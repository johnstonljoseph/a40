{
  "student": {
    "results": {
      "arc_challenge": {
        "alias": "arc_challenge",
        "acc,none": 0.5051194539249146,
        "acc_stderr,none": 0.014610624890309135,
        "acc_norm,none": 0.5358361774744027,
        "acc_norm_stderr,none": 0.014573813664735622
      },
      "gsm8k": {
        "alias": "gsm8k",
        "exact_match,strict-match": 0.8157695223654283,
        "exact_match_stderr,strict-match": 0.010678414428555008,
        "exact_match,flexible-extract": 0.8142532221379833,
        "exact_match_stderr,flexible-extract": 0.010712298902729069
      },
      "hellaswag": {
        "alias": "hellaswag",
        "acc,none": 0.5296753634734117,
        "acc_stderr,none": 0.004980985384152798,
        "acc_norm,none": 0.7018522206731727,
        "acc_norm_stderr,none": 0.004565098421085211
      },
      "humaneval": {
        "alias": "humaneval",
        "pass@1,create_test": 0.4817073170731707,
        "pass@1_stderr,create_test": 0.03913680408588943
      },
      "mbpp": {
        "alias": "mbpp",
        "pass_at_1,none": 0.416,
        "pass_at_1_stderr,none": 0.02206494331392887
      },
      "mmlu": {
        "acc,none": 0.567511750462897,
        "acc_stderr,none": 0.004007318131661192,
        "alias": "mmlu"
      },
      "mmlu_humanities": {
        "acc,none": 0.5013815090329437,
        "acc_stderr,none": 0.0069090684076178665,
        "alias": " - humanities"
      },
      "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.40476190476190477,
        "acc_stderr,none": 0.043902592653775656
      },
      "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.6727272727272727,
        "acc_stderr,none": 0.036639749943912406
      },
      "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.6911764705882353,
        "acc_stderr,none": 0.03242661719827215
      },
      "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.729957805907173,
        "acc_stderr,none": 0.02890072190629346
      },
      "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.7355371900826446,
        "acc_stderr,none": 0.04026187527591209
      },
      "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.6851851851851852,
        "acc_stderr,none": 0.0448993107359131
      },
      "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.6687116564417178,
        "acc_stderr,none": 0.03697983910025588
      },
      "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.615606936416185,
        "acc_stderr,none": 0.0261896669662721
      },
      "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.28044692737430166,
        "acc_stderr,none": 0.015024083883323009
      },
      "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.617363344051447,
        "acc_stderr,none": 0.027604689028581996
      },
      "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.6574074074074074,
        "acc_stderr,none": 0.0264061459736257
      },
      "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4048239895697523,
        "acc_stderr,none": 0.012536743830953968
      },
      "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7076023391812866,
        "acc_stderr,none": 0.03488647713457921
      },
      "mmlu_other": {
        "acc,none": 0.6202124235597039,
        "acc_stderr,none": 0.008471233211286898,
        "alias": " - other"
      },
      "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.65,
        "acc_stderr,none": 0.04793724854411023
      },
      "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.5886792452830188,
        "acc_stderr,none": 0.030285009259009833
      },
      "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6011560693641619,
        "acc_stderr,none": 0.03733626655383514
      },
      "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.4,
        "acc_stderr,none": 0.0492365963917331
      },
      "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6233183856502242,
        "acc_stderr,none": 0.03252113489929187
      },
      "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7281553398058253,
        "acc_stderr,none": 0.04405268024140923
      },
      "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8162393162393162,
        "acc_stderr,none": 0.0253721396717229
      },
      "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.6,
        "acc_stderr,none": 0.0492365963917331
      },
      "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.7177522349936143,
        "acc_stderr,none": 0.01609530296987855
      },
      "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.6503267973856209,
        "acc_stderr,none": 0.027305308076274667
      },
      "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.41843971631205673,
        "acc_stderr,none": 0.02942799403941995
      },
      "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5294117647058824,
        "acc_stderr,none": 0.030320243265004078
      },
      "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.4457831325301205,
        "acc_stderr,none": 0.03869543323472097
      },
      "mmlu_social_sciences": {
        "acc,none": 0.6564835879103023,
        "acc_stderr,none": 0.008400038811740519,
        "alias": " - social sciences"
      },
      "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.42105263157894735,
        "acc_stderr,none": 0.04644602091222323
      },
      "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7575757575757576,
        "acc_stderr,none": 0.030532892233932022
      },
      "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.772020725388601,
        "acc_stderr,none": 0.030276909945178256
      },
      "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5358974358974359,
        "acc_stderr,none": 0.025285585990017807
      },
      "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6386554621848739,
        "acc_stderr,none": 0.031204691225150072
      },
      "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7596330275229358,
        "acc_stderr,none": 0.01832060732096402
      },
      "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.6412213740458015,
        "acc_stderr,none": 0.04206739313864908
      },
      "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.5833333333333334,
        "acc_stderr,none": 0.01994491413687358
      },
      "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6090909090909091,
        "acc_stderr,none": 0.046737523336702363
      },
      "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.6775510204081633,
        "acc_stderr,none": 0.029923100563683976
      },
      "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7412935323383084,
        "acc_stderr,none": 0.030965903123573005
      },
      "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.75,
        "acc_stderr,none": 0.04351941398892446
      },
      "mmlu_stem": {
        "acc,none": 0.5274341896606407,
        "acc_stderr,none": 0.00867967925958176,
        "alias": " - stem"
      },
      "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.38,
        "acc_stderr,none": 0.04878317312145634
      },
      "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.5185185185185185,
        "acc_stderr,none": 0.043163785995113245
      },
      "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.625,
        "acc_stderr,none": 0.039397364351956274
      },
      "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.7013888888888888,
        "acc_stderr,none": 0.03827052357950753
      },
      "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.45,
        "acc_stderr,none": 0.05
      },
      "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.52,
        "acc_stderr,none": 0.05021167315686783
      },
      "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.41,
        "acc_stderr,none": 0.04943110704237104
      },
      "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.4215686274509804,
        "acc_stderr,none": 0.04913595201274502
      },
      "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.66,
        "acc_stderr,none": 0.04760952285695234
      },
      "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.574468085106383,
        "acc_stderr,none": 0.03232146916224468
      },
      "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6137931034482759,
        "acc_stderr,none": 0.04057324734419032
      },
      "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.4947089947089947,
        "acc_stderr,none": 0.025749868288556587
      },
      "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7129032258064516,
        "acc_stderr,none": 0.02573654274559456
      },
      "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5270935960591133,
        "acc_stderr,none": 0.03512819077876112
      },
      "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.68,
        "acc_stderr,none": 0.046882617226215076
      },
      "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.362962962962963,
        "acc_stderr,none": 0.02931820364520686
      },
      "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.423841059602649,
        "acc_stderr,none": 0.04034846678603395
      },
      "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.49074074074074076,
        "acc_stderr,none": 0.034093869469926964
      },
      "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.33035714285714285,
        "acc_stderr,none": 0.04464285714285714
      },
      "winogrande": {
        "alias": "winogrande",
        "acc,none": 0.6566692975532754,
        "acc_stderr,none": 0.013344823185358066
      }
    },
    "aggregate": null,
    "config": {
      "model": "/workspace/a40/checkpoints/r5-159",
      "model_args": null,
      "model_num_parameters": 7298846944,
      "model_dtype": "torch.bfloat16",
      "model_revision": "main",
      "model_sha": "",
      "batch_size": 16,
      "batch_sizes": [],
      "device": null,
      "use_cache": null,
      "limit": null,
      "bootstrap_iters": 100000,
      "gen_kwargs": null,
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    }
  },
  "teacher": {
    "results": {
      "arc_challenge": {
        "alias": "arc_challenge",
        "acc,none": 0.5247440273037542,
        "acc_stderr,none": 0.014593487694937702,
        "acc_norm,none": 0.5426621160409556,
        "acc_norm_stderr,none": 0.014558106543923992
      },
      "gsm8k": {
        "alias": "gsm8k",
        "exact_match,strict-match": 0.8362395754359363,
        "exact_match_stderr,strict-match": 0.01019323721442098,
        "exact_match,flexible-extract": 0.7119029567854435,
        "exact_match_stderr,flexible-extract": 0.012474469737197916
      },
      "hellaswag": {
        "alias": "hellaswag",
        "acc,none": 0.5569607647878908,
        "acc_stderr,none": 0.004957296691391718,
        "acc_norm,none": 0.7421828321051583,
        "acc_norm_stderr,none": 0.0043653883515635365
      },
      "humaneval": {
        "alias": "humaneval",
        "pass@1,create_test": 0.4024390243902439,
        "pass@1_stderr,create_test": 0.03841026959095089
      },
      "mbpp": {
        "alias": "mbpp",
        "pass_at_1,none": 0.426,
        "pass_at_1_stderr,none": 0.022136577335085637
      },
      "mmlu": {
        "acc,none": 0.5950719270759152,
        "acc_stderr,none": 0.003948728665668892,
        "alias": "mmlu"
      },
      "mmlu_humanities": {
        "acc,none": 0.5243358129649309,
        "acc_stderr,none": 0.00685876459365114,
        "alias": " - humanities"
      },
      "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.42063492063492064,
        "acc_stderr,none": 0.044154382267437474
      },
      "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7151515151515152,
        "acc_stderr,none": 0.03524390844511785
      },
      "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.75,
        "acc_stderr,none": 0.03039153369274154
      },
      "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7848101265822784,
        "acc_stderr,none": 0.026750826994676173
      },
      "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.7768595041322314,
        "acc_stderr,none": 0.038007544752287334
      },
      "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7222222222222222,
        "acc_stderr,none": 0.04330043749650743
      },
      "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7055214723926381,
        "acc_stderr,none": 0.03581165790474083
      },
      "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.6329479768786127,
        "acc_stderr,none": 0.025950054337654085
      },
      "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3016759776536313,
        "acc_stderr,none": 0.015350767572220217
      },
      "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.6302250803858521,
        "acc_stderr,none": 0.027417996705631043
      },
      "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.6851851851851852,
        "acc_stderr,none": 0.025842248700902248
      },
      "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4178617992177314,
        "acc_stderr,none": 0.012596744108998465
      },
      "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7134502923976608,
        "acc_stderr,none": 0.03467826685703828
      },
      "mmlu_other": {
        "acc,none": 0.6488574187318957,
        "acc_stderr,none": 0.008284804105574573,
        "alias": " - other"
      },
      "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.63,
        "acc_stderr,none": 0.048523658709390974
      },
      "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6528301886792452,
        "acc_stderr,none": 0.02930010170554966
      },
      "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6127167630057804,
        "acc_stderr,none": 0.03714325906302067
      },
      "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001973
      },
      "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6188340807174888,
        "acc_stderr,none": 0.0325962511841683
      },
      "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7572815533980582,
        "acc_stderr,none": 0.04245022486384496
      },
      "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8589743589743589,
        "acc_stderr,none": 0.022801382534597486
      },
      "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.62,
        "acc_stderr,none": 0.04878317312145634
      },
      "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.7586206896551724,
        "acc_stderr,none": 0.015302380123542047
      },
      "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.673202614379085,
        "acc_stderr,none": 0.026857294663281482
      },
      "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4432624113475177,
        "acc_stderr,none": 0.02963483847376595
      },
      "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5698529411764706,
        "acc_stderr,none": 0.030074971917302858
      },
      "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.4578313253012048,
        "acc_stderr,none": 0.038786267710023595
      },
      "mmlu_social_sciences": {
        "acc,none": 0.6919077023074424,
        "acc_stderr,none": 0.008178904583091099,
        "alias": " - social sciences"
      },
      "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.5087719298245614,
        "acc_stderr,none": 0.0470288043204961
      },
      "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.803030303030303,
        "acc_stderr,none": 0.02833560973246333
      },
      "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.7927461139896373,
        "acc_stderr,none": 0.02925282329180367
      },
      "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5794871794871795,
        "acc_stderr,none": 0.02502861027671089
      },
      "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.7058823529411765,
        "acc_stderr,none": 0.029597329730978096
      },
      "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7871559633027523,
        "acc_stderr,none": 0.01754937638931373
      },
      "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7022900763358778,
        "acc_stderr,none": 0.04010358942462197
      },
      "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6013071895424836,
        "acc_stderr,none": 0.019808281317449855
      },
      "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7272727272727273,
        "acc_stderr,none": 0.04265792110940591
      },
      "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.6775510204081633,
        "acc_stderr,none": 0.029923100563683976
      },
      "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7512437810945274,
        "acc_stderr,none": 0.030567675938916686
      },
      "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.79,
        "acc_stderr,none": 0.040936018074033236
      },
      "mmlu_stem": {
        "acc,none": 0.5531240088804313,
        "acc_stderr,none": 0.008611524137017629,
        "alias": " - stem"
      },
      "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.42,
        "acc_stderr,none": 0.04960449637488583
      },
      "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.5185185185185185,
        "acc_stderr,none": 0.043163785995113245
      },
      "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.7039473684210527,
        "acc_stderr,none": 0.037150621549989084
      },
      "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.75,
        "acc_stderr,none": 0.03621034121889507
      },
      "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.47,
        "acc_stderr,none": 0.05016135580465919
      },
      "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.53,
        "acc_stderr,none": 0.05016135580465919
      },
      "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.44,
        "acc_stderr,none": 0.049888765156985884
      },
      "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.39215686274509803,
        "acc_stderr,none": 0.04858083574266346
      },
      "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.68,
        "acc_stderr,none": 0.046882617226215076
      },
      "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.6170212765957447,
        "acc_stderr,none": 0.03177821250236923
      },
      "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6275862068965518,
        "acc_stderr,none": 0.0402873153294756
      },
      "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.5211640211640212,
        "acc_stderr,none": 0.025728230952130723
      },
      "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7451612903225806,
        "acc_stderr,none": 0.02479011845933226
      },
      "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5270935960591133,
        "acc_stderr,none": 0.03512819077876112
      },
      "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.72,
        "acc_stderr,none": 0.045126085985421296
      },
      "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.37777777777777777,
        "acc_stderr,none": 0.029560707392465774
      },
      "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.4370860927152318,
        "acc_stderr,none": 0.040500357222306375
      },
      "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.4861111111111111,
        "acc_stderr,none": 0.03408655867977753
      },
      "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.4375,
        "acc_stderr,none": 0.04708567521880525
      },
      "winogrande": {
        "alias": "winogrande",
        "acc,none": 0.6787687450670876,
        "acc_stderr,none": 0.013123599324558302
      }
    },
    "aggregate": null,
    "config": {
      "model": "/workspace/.hf_home/hub/models--allenai--Olmo-3-7B-Think/snapshots/61748f8c6c5c88533dba6560f211d4d1be51b31a",
      "model_args": null,
      "model_num_parameters": 7298011136,
      "model_dtype": "torch.bfloat16",
      "model_revision": "main",
      "model_sha": "",
      "batch_size": 16,
      "batch_sizes": [],
      "device": null,
      "use_cache": null,
      "limit": null,
      "bootstrap_iters": 100000,
      "gen_kwargs": null,
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    }
  }
}