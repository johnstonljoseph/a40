{
  "student": {
    "results": {
      "arc_challenge": {
        "alias": "arc_challenge",
        "acc,none": 0.40187713310580203,
        "acc_stderr,none": 0.014327268614578337,
        "acc_norm,none": 0.4598976109215017,
        "acc_norm_stderr,none": 0.01456431885692496
      },
      "gsm8k": {
        "alias": "gsm8k",
        "exact_match,strict-match": 0.7194844579226687,
        "exact_match_stderr,strict-match": 0.012374608490929568,
        "exact_match,flexible-extract": 0.7202426080363912,
        "exact_match_stderr,flexible-extract": 0.01236438401673532
      },
      "hellaswag": {
        "alias": "hellaswag",
        "acc,none": 0.45956980681139215,
        "acc_stderr,none": 0.004973442060741616,
        "acc_norm,none": 0.6073491336387173,
        "acc_norm_stderr,none": 0.004873421833291711
      },
      "humaneval": {
        "alias": "humaneval",
        "pass@1,create_test": 0.524390243902439,
        "pass@1_stderr,create_test": 0.03911639983703664
      },
      "mbpp": {
        "alias": "mbpp",
        "pass_at_1,none": 0.388,
        "pass_at_1_stderr,none": 0.021814300984787635
      },
      "mmlu": {
        "acc,none": 0.5550491382993875,
        "acc_stderr,none": 0.0040290534979545165,
        "alias": "mmlu"
      },
      "mmlu_humanities": {
        "acc,none": 0.49287991498405953,
        "acc_stderr,none": 0.0069459588035632605,
        "alias": " - humanities"
      },
      "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.38095238095238093,
        "acc_stderr,none": 0.043435254289490965
      },
      "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.6606060606060606,
        "acc_stderr,none": 0.03697442205031597
      },
      "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.7009803921568627,
        "acc_stderr,none": 0.032133257173736156
      },
      "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7383966244725738,
        "acc_stderr,none": 0.028609516716994906
      },
      "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.6859504132231405,
        "acc_stderr,none": 0.04236964753041019
      },
      "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.6851851851851852,
        "acc_stderr,none": 0.0448993107359131
      },
      "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.6932515337423313,
        "acc_stderr,none": 0.036230899157241474
      },
      "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.5809248554913294,
        "acc_stderr,none": 0.026564178111422608
      },
      "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.28268156424581004,
        "acc_stderr,none": 0.015060381730018045
      },
      "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.6077170418006431,
        "acc_stderr,none": 0.027731258647012022
      },
      "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.6018518518518519,
        "acc_stderr,none": 0.02723741509459248
      },
      "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4061277705345502,
        "acc_stderr,none": 0.012543154588412882
      },
      "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.6608187134502924,
        "acc_stderr,none": 0.03631053496488902
      },
      "mmlu_other": {
        "acc,none": 0.6060508529127776,
        "acc_stderr,none": 0.00850993973451858,
        "alias": " - other"
      },
      "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.58,
        "acc_stderr,none": 0.04960449637488582
      },
      "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.5962264150943396,
        "acc_stderr,none": 0.03019761160019793
      },
      "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.5549132947976878,
        "acc_stderr,none": 0.037894017602836505
      },
      "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.36,
        "acc_stderr,none": 0.048241815132442176
      },
      "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.600896860986547,
        "acc_stderr,none": 0.03286745312567959
      },
      "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7087378640776699,
        "acc_stderr,none": 0.044986763205729245
      },
      "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.7948717948717948,
        "acc_stderr,none": 0.02645350805404033
      },
      "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.61,
        "acc_stderr,none": 0.04902071300001973
      },
      "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.7241379310344828,
        "acc_stderr,none": 0.01598281477469557
      },
      "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.6045751633986928,
        "acc_stderr,none": 0.027996723180631466
      },
      "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.3971631205673759,
        "acc_stderr,none": 0.029189805673587126
      },
      "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5330882352941176,
        "acc_stderr,none": 0.03030625772246828
      },
      "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.43373493975903615,
        "acc_stderr,none": 0.03858158940685518
      },
      "mmlu_social_sciences": {
        "acc,none": 0.6428339291517712,
        "acc_stderr,none": 0.008476195558134936,
        "alias": " - social sciences"
      },
      "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.41228070175438597,
        "acc_stderr,none": 0.046306532033665936
      },
      "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.7121212121212122,
        "acc_stderr,none": 0.03225883512300997
      },
      "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.7512953367875648,
        "acc_stderr,none": 0.03119584087770025
      },
      "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5128205128205128,
        "acc_stderr,none": 0.025342671293807247
      },
      "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.6638655462184874,
        "acc_stderr,none": 0.030684737115135356
      },
      "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7357798165137615,
        "acc_stderr,none": 0.018904164171510297
      },
      "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.6564885496183206,
        "acc_stderr,none": 0.041649760719448814
      },
      "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.5604575163398693,
        "acc_stderr,none": 0.020079420408087908
      },
      "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.6272727272727273,
        "acc_stderr,none": 0.04631381319425461
      },
      "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.6530612244897959,
        "acc_stderr,none": 0.03047252602672653
      },
      "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7512437810945274,
        "acc_stderr,none": 0.030567675938916686
      },
      "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.77,
        "acc_stderr,none": 0.042295258468165065
      },
      "mmlu_stem": {
        "acc,none": 0.5118934348239772,
        "acc_stderr,none": 0.008706332206537992,
        "alias": " - stem"
      },
      "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.37,
        "acc_stderr,none": 0.048523658709390974
      },
      "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.45925925925925926,
        "acc_stderr,none": 0.04304979692464244
      },
      "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.5855263157894737,
        "acc_stderr,none": 0.04008973785779205
      },
      "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.6666666666666666,
        "acc_stderr,none": 0.03942082639927217
      },
      "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.46,
        "acc_stderr,none": 0.05009082659620332
      },
      "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.54,
        "acc_stderr,none": 0.05009082659620331
      },
      "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.44,
        "acc_stderr,none": 0.049888765156985884
      },
      "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.3627450980392157,
        "acc_stderr,none": 0.047840607041056527
      },
      "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.66,
        "acc_stderr,none": 0.04760952285695234
      },
      "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.5404255319148936,
        "acc_stderr,none": 0.03257901482099833
      },
      "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.5793103448275863,
        "acc_stderr,none": 0.0411391498118926
      },
      "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.4656084656084656,
        "acc_stderr,none": 0.025690321762493876
      },
      "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7,
        "acc_stderr,none": 0.026069362295335054
      },
      "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5714285714285714,
        "acc_stderr,none": 0.03481904844438804
      },
      "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.63,
        "acc_stderr,none": 0.048523658709390974
      },
      "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.362962962962963,
        "acc_stderr,none": 0.02931820364520686
      },
      "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.3708609271523179,
        "acc_stderr,none": 0.039439666991836285
      },
      "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.49537037037037035,
        "acc_stderr,none": 0.034098255191635805
      },
      "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.3482142857142857,
        "acc_stderr,none": 0.04521829902833581
      },
      "winogrande": {
        "alias": "winogrande",
        "acc,none": 0.6235201262825573,
        "acc_stderr,none": 0.01361693196066715
      }
    },
    "aggregate": null,
    "config": {
      "model": "/workspace/a40/checkpoints/r2-408",
      "model_args": null,
      "model_num_parameters": 7298846816,
      "model_dtype": "torch.bfloat16",
      "model_revision": "main",
      "model_sha": "",
      "batch_size": 16,
      "batch_sizes": [],
      "device": null,
      "use_cache": null,
      "limit": null,
      "bootstrap_iters": 100000,
      "gen_kwargs": null,
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    }
  },
  "teacher": {
    "results": {
      "arc_challenge": {
        "alias": "arc_challenge",
        "acc,none": 0.5247440273037542,
        "acc_stderr,none": 0.014593487694937702,
        "acc_norm,none": 0.5426621160409556,
        "acc_norm_stderr,none": 0.014558106543923992
      },
      "gsm8k": {
        "alias": "gsm8k",
        "exact_match,strict-match": 0.8362395754359363,
        "exact_match_stderr,strict-match": 0.01019323721442098,
        "exact_match,flexible-extract": 0.7119029567854435,
        "exact_match_stderr,flexible-extract": 0.012474469737197916
      },
      "hellaswag": {
        "alias": "hellaswag",
        "acc,none": 0.5569607647878908,
        "acc_stderr,none": 0.004957296691391718,
        "acc_norm,none": 0.7421828321051583,
        "acc_norm_stderr,none": 0.0043653883515635365
      },
      "humaneval": {
        "alias": "humaneval",
        "pass@1,create_test": 0.4024390243902439,
        "pass@1_stderr,create_test": 0.03841026959095089
      },
      "mbpp": {
        "alias": "mbpp",
        "pass_at_1,none": 0.426,
        "pass_at_1_stderr,none": 0.022136577335085637
      },
      "mmlu": {
        "acc,none": 0.5950719270759152,
        "acc_stderr,none": 0.003948728665668892,
        "alias": "mmlu"
      },
      "mmlu_humanities": {
        "acc,none": 0.5243358129649309,
        "acc_stderr,none": 0.00685876459365114,
        "alias": " - humanities"
      },
      "mmlu_formal_logic": {
        "alias": "  - formal_logic",
        "acc,none": 0.42063492063492064,
        "acc_stderr,none": 0.044154382267437474
      },
      "mmlu_high_school_european_history": {
        "alias": "  - high_school_european_history",
        "acc,none": 0.7151515151515152,
        "acc_stderr,none": 0.03524390844511785
      },
      "mmlu_high_school_us_history": {
        "alias": "  - high_school_us_history",
        "acc,none": 0.75,
        "acc_stderr,none": 0.03039153369274154
      },
      "mmlu_high_school_world_history": {
        "alias": "  - high_school_world_history",
        "acc,none": 0.7848101265822784,
        "acc_stderr,none": 0.026750826994676173
      },
      "mmlu_international_law": {
        "alias": "  - international_law",
        "acc,none": 0.7768595041322314,
        "acc_stderr,none": 0.038007544752287334
      },
      "mmlu_jurisprudence": {
        "alias": "  - jurisprudence",
        "acc,none": 0.7222222222222222,
        "acc_stderr,none": 0.04330043749650743
      },
      "mmlu_logical_fallacies": {
        "alias": "  - logical_fallacies",
        "acc,none": 0.7055214723926381,
        "acc_stderr,none": 0.03581165790474083
      },
      "mmlu_moral_disputes": {
        "alias": "  - moral_disputes",
        "acc,none": 0.6329479768786127,
        "acc_stderr,none": 0.025950054337654085
      },
      "mmlu_moral_scenarios": {
        "alias": "  - moral_scenarios",
        "acc,none": 0.3016759776536313,
        "acc_stderr,none": 0.015350767572220217
      },
      "mmlu_philosophy": {
        "alias": "  - philosophy",
        "acc,none": 0.6302250803858521,
        "acc_stderr,none": 0.027417996705631043
      },
      "mmlu_prehistory": {
        "alias": "  - prehistory",
        "acc,none": 0.6851851851851852,
        "acc_stderr,none": 0.025842248700902248
      },
      "mmlu_professional_law": {
        "alias": "  - professional_law",
        "acc,none": 0.4178617992177314,
        "acc_stderr,none": 0.012596744108998465
      },
      "mmlu_world_religions": {
        "alias": "  - world_religions",
        "acc,none": 0.7134502923976608,
        "acc_stderr,none": 0.03467826685703828
      },
      "mmlu_other": {
        "acc,none": 0.6488574187318957,
        "acc_stderr,none": 0.008284804105574573,
        "alias": " - other"
      },
      "mmlu_business_ethics": {
        "alias": "  - business_ethics",
        "acc,none": 0.63,
        "acc_stderr,none": 0.048523658709390974
      },
      "mmlu_clinical_knowledge": {
        "alias": "  - clinical_knowledge",
        "acc,none": 0.6528301886792452,
        "acc_stderr,none": 0.02930010170554966
      },
      "mmlu_college_medicine": {
        "alias": "  - college_medicine",
        "acc,none": 0.6127167630057804,
        "acc_stderr,none": 0.03714325906302067
      },
      "mmlu_global_facts": {
        "alias": "  - global_facts",
        "acc,none": 0.39,
        "acc_stderr,none": 0.04902071300001973
      },
      "mmlu_human_aging": {
        "alias": "  - human_aging",
        "acc,none": 0.6188340807174888,
        "acc_stderr,none": 0.0325962511841683
      },
      "mmlu_management": {
        "alias": "  - management",
        "acc,none": 0.7572815533980582,
        "acc_stderr,none": 0.04245022486384496
      },
      "mmlu_marketing": {
        "alias": "  - marketing",
        "acc,none": 0.8589743589743589,
        "acc_stderr,none": 0.022801382534597486
      },
      "mmlu_medical_genetics": {
        "alias": "  - medical_genetics",
        "acc,none": 0.62,
        "acc_stderr,none": 0.04878317312145634
      },
      "mmlu_miscellaneous": {
        "alias": "  - miscellaneous",
        "acc,none": 0.7586206896551724,
        "acc_stderr,none": 0.015302380123542047
      },
      "mmlu_nutrition": {
        "alias": "  - nutrition",
        "acc,none": 0.673202614379085,
        "acc_stderr,none": 0.026857294663281482
      },
      "mmlu_professional_accounting": {
        "alias": "  - professional_accounting",
        "acc,none": 0.4432624113475177,
        "acc_stderr,none": 0.02963483847376595
      },
      "mmlu_professional_medicine": {
        "alias": "  - professional_medicine",
        "acc,none": 0.5698529411764706,
        "acc_stderr,none": 0.030074971917302858
      },
      "mmlu_virology": {
        "alias": "  - virology",
        "acc,none": 0.4578313253012048,
        "acc_stderr,none": 0.038786267710023595
      },
      "mmlu_social_sciences": {
        "acc,none": 0.6919077023074424,
        "acc_stderr,none": 0.008178904583091099,
        "alias": " - social sciences"
      },
      "mmlu_econometrics": {
        "alias": "  - econometrics",
        "acc,none": 0.5087719298245614,
        "acc_stderr,none": 0.0470288043204961
      },
      "mmlu_high_school_geography": {
        "alias": "  - high_school_geography",
        "acc,none": 0.803030303030303,
        "acc_stderr,none": 0.02833560973246333
      },
      "mmlu_high_school_government_and_politics": {
        "alias": "  - high_school_government_and_politics",
        "acc,none": 0.7927461139896373,
        "acc_stderr,none": 0.02925282329180367
      },
      "mmlu_high_school_macroeconomics": {
        "alias": "  - high_school_macroeconomics",
        "acc,none": 0.5794871794871795,
        "acc_stderr,none": 0.02502861027671089
      },
      "mmlu_high_school_microeconomics": {
        "alias": "  - high_school_microeconomics",
        "acc,none": 0.7058823529411765,
        "acc_stderr,none": 0.029597329730978096
      },
      "mmlu_high_school_psychology": {
        "alias": "  - high_school_psychology",
        "acc,none": 0.7871559633027523,
        "acc_stderr,none": 0.01754937638931373
      },
      "mmlu_human_sexuality": {
        "alias": "  - human_sexuality",
        "acc,none": 0.7022900763358778,
        "acc_stderr,none": 0.04010358942462197
      },
      "mmlu_professional_psychology": {
        "alias": "  - professional_psychology",
        "acc,none": 0.6013071895424836,
        "acc_stderr,none": 0.019808281317449855
      },
      "mmlu_public_relations": {
        "alias": "  - public_relations",
        "acc,none": 0.7272727272727273,
        "acc_stderr,none": 0.04265792110940591
      },
      "mmlu_security_studies": {
        "alias": "  - security_studies",
        "acc,none": 0.6775510204081633,
        "acc_stderr,none": 0.029923100563683976
      },
      "mmlu_sociology": {
        "alias": "  - sociology",
        "acc,none": 0.7512437810945274,
        "acc_stderr,none": 0.030567675938916686
      },
      "mmlu_us_foreign_policy": {
        "alias": "  - us_foreign_policy",
        "acc,none": 0.79,
        "acc_stderr,none": 0.040936018074033236
      },
      "mmlu_stem": {
        "acc,none": 0.5531240088804313,
        "acc_stderr,none": 0.008611524137017629,
        "alias": " - stem"
      },
      "mmlu_abstract_algebra": {
        "alias": "  - abstract_algebra",
        "acc,none": 0.42,
        "acc_stderr,none": 0.04960449637488583
      },
      "mmlu_anatomy": {
        "alias": "  - anatomy",
        "acc,none": 0.5185185185185185,
        "acc_stderr,none": 0.043163785995113245
      },
      "mmlu_astronomy": {
        "alias": "  - astronomy",
        "acc,none": 0.7039473684210527,
        "acc_stderr,none": 0.037150621549989084
      },
      "mmlu_college_biology": {
        "alias": "  - college_biology",
        "acc,none": 0.75,
        "acc_stderr,none": 0.03621034121889507
      },
      "mmlu_college_chemistry": {
        "alias": "  - college_chemistry",
        "acc,none": 0.47,
        "acc_stderr,none": 0.05016135580465919
      },
      "mmlu_college_computer_science": {
        "alias": "  - college_computer_science",
        "acc,none": 0.53,
        "acc_stderr,none": 0.05016135580465919
      },
      "mmlu_college_mathematics": {
        "alias": "  - college_mathematics",
        "acc,none": 0.44,
        "acc_stderr,none": 0.049888765156985884
      },
      "mmlu_college_physics": {
        "alias": "  - college_physics",
        "acc,none": 0.39215686274509803,
        "acc_stderr,none": 0.04858083574266346
      },
      "mmlu_computer_security": {
        "alias": "  - computer_security",
        "acc,none": 0.68,
        "acc_stderr,none": 0.046882617226215076
      },
      "mmlu_conceptual_physics": {
        "alias": "  - conceptual_physics",
        "acc,none": 0.6170212765957447,
        "acc_stderr,none": 0.03177821250236923
      },
      "mmlu_electrical_engineering": {
        "alias": "  - electrical_engineering",
        "acc,none": 0.6275862068965518,
        "acc_stderr,none": 0.0402873153294756
      },
      "mmlu_elementary_mathematics": {
        "alias": "  - elementary_mathematics",
        "acc,none": 0.5211640211640212,
        "acc_stderr,none": 0.025728230952130723
      },
      "mmlu_high_school_biology": {
        "alias": "  - high_school_biology",
        "acc,none": 0.7451612903225806,
        "acc_stderr,none": 0.02479011845933226
      },
      "mmlu_high_school_chemistry": {
        "alias": "  - high_school_chemistry",
        "acc,none": 0.5270935960591133,
        "acc_stderr,none": 0.03512819077876112
      },
      "mmlu_high_school_computer_science": {
        "alias": "  - high_school_computer_science",
        "acc,none": 0.72,
        "acc_stderr,none": 0.045126085985421296
      },
      "mmlu_high_school_mathematics": {
        "alias": "  - high_school_mathematics",
        "acc,none": 0.37777777777777777,
        "acc_stderr,none": 0.029560707392465774
      },
      "mmlu_high_school_physics": {
        "alias": "  - high_school_physics",
        "acc,none": 0.4370860927152318,
        "acc_stderr,none": 0.040500357222306375
      },
      "mmlu_high_school_statistics": {
        "alias": "  - high_school_statistics",
        "acc,none": 0.4861111111111111,
        "acc_stderr,none": 0.03408655867977753
      },
      "mmlu_machine_learning": {
        "alias": "  - machine_learning",
        "acc,none": 0.4375,
        "acc_stderr,none": 0.04708567521880525
      },
      "winogrande": {
        "alias": "winogrande",
        "acc,none": 0.6787687450670876,
        "acc_stderr,none": 0.013123599324558302
      }
    },
    "aggregate": null,
    "config": {
      "model": "/workspace/.hf_home/hub/models--allenai--Olmo-3-7B-Think/snapshots/61748f8c6c5c88533dba6560f211d4d1be51b31a",
      "model_args": null,
      "model_num_parameters": 7298011136,
      "model_dtype": "torch.bfloat16",
      "model_revision": "main",
      "model_sha": "",
      "batch_size": 16,
      "batch_sizes": [],
      "device": null,
      "use_cache": null,
      "limit": null,
      "bootstrap_iters": 100000,
      "gen_kwargs": null,
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    }
  }
}