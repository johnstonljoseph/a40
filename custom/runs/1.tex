
Now LR is not multiplied by KL.
and now using quantizer with weights, because we think this should work.

step 330 we're at blend 0.75, before we were past blend 1. why?
same batch size, almost same LR, but maybe when KL goes up it with constant LR it takes longer to come back down than with higher LR.
we also doubled MSE weight, but that's negligible in total loss.
oh, we changed dataset distribution from 0.6/2*0.2 to 0.8/2*0.1.
we increased shuffle buffer size by 10x.

it hit blend 1.0, not sure at what step. we're not at 490. saving checkpoint with kl 0.09.
next should probably lower LR.

630, back up to 0.13, so I halve LR to 4e-5.
680, went back down but now very recently up to .26, so I set LR to 2e-5.
730: dipped below 0.1 again
891: back up, so LR to 1e-5

got to step 1096 then gave up, even with 1e-6 KL still jumps around. 


CHOSEN_ACTIVATION = IdentityActivation
# GATE_UP_QUANTIZER = QuantLinearWithScales
# DOWN_QUANTIZER = QuantLinearWithScales
GATE_UP_QUANTIZER = QuantLinearWithWeights
DOWN_QUANTIZER = QuantLinearWithWeights

@dataclass
class Config:
    batch_size: int = 6
    seq_len: int = 1024
    accumulate_steps: int = 8
    lr: float = 8e-5
    device: str = "cuda"
    dtype: str = "bfloat16"
    base_path: str = "/workspace/.hf_home/hub/models--allenai--Olmo-3-7B-Think"
    output_dir: str = str(DIR.parent / "checkpoints" / "student_final")
    dataset_sft: Optional[str] = "allenai/Dolci-Think-SFT-7B"
    dataset_dpo: Optional[str] = "allenai/Dolci-Think-DPO-7B"
    dataset_rl: Optional[str] = "allenai/Dolci-Think-RL-7B"
    dataset_ratio_sft: float = 0.8
    dataset_ratio_dpo: float = 0.1
    dataset_ratio_rl: float = 0.1
    shuffle_buffer_size: int = 10000
    seed: int = 42
    num_workers: int = 1
    compile: bool = True
    blend_steps: int = 20
    kl_threshold: float = 0.1
    hidden_mse_weight: float = 2.0
    kl_weight: float = 1.0
    weight_decay: float = 0.05
    act_fn_lr_mult: float = 5.0
    log_act_s_lr_mult: float = 20.0
    log_diag_s_lr_mult: float = 10.0
    log_weight_s_lr_mult: float = 1.0
